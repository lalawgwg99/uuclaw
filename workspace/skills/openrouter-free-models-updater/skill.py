#!/usr/bin/env python3
"""
OpenRouter å…è²»æ¨¡å‹åˆ†æå™¨
æ¯å¤©è‡ªå‹•æª¢æŸ¥å…è²»æ¨¡å‹ä¸¦é€šé Telegram ç™¼é€åˆ†æå ±å‘Š
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime

try:
    import requests
except ImportError:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ requests å‡½å¼åº«")
    print("è«‹åŸ·è¡Œï¼špip3 install requests")
    sys.exit(1)


def fetch_free_models():
    """å¾ OpenRouter API ç²å–å…è²»æ¨¡å‹åˆ—è¡¨ï¼ˆå¸¶å¿«å–ï¼Œç¯€çœ API å‘¼å«èˆ‡ tokenï¼‰"""
    import time
    cache_path = Path.home() / ".openclaw" / "free-models-cache.json"
    cache_ttl_ms = 60 * 60 * 1000  # 1 å°æ™‚

    # å˜—è©¦è®€å–å¿«å–
    try:
        if cache_path.exists():
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            last_check_ts = datetime.fromisoformat(cache.get("last_check", "")).timestamp() * 1000
            if (time.time() * 1000 - last_check_ts) < cache_ttl_ms:
                models = cache.get("models", [])
                print(f"[cache] ä½¿ç”¨å…è²»æ¨¡å‹å¿«å–ï¼ˆ{len(models)} å€‹æ¨¡å‹ï¼ŒTTL å‰©é¤˜ {int((cache_ttl_ms - (time.time()*1000 - last_check_ts))/60000)} åˆ†ï¼‰")
                return models
    except Exception as e:
        print(f"[cache] è®€å–å¿«å–å¤±æ•—ï¼š{e}ï¼Œå°‡é‡æ–°å–å¾—")

    # è‹¥ç„¡å¿«å–æˆ–å¿«å–éæœŸï¼Œå¾ API å–å¾—
    print("[cache] å¿«å–æœªå‘½ä¸­ï¼Œæ­£åœ¨å¾ OpenRouter ç²å–æœ€æ–°æ¨¡å‹åˆ—è¡¨...")
    url = "https://openrouter.ai/api/v1/models"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        models = data.get("data", [])
        
        # ç¯©é¸å…è²»æ¨¡å‹
        free_models = []
        for model in models:
            pricing = model.get("pricing", {})
            prompt_price = float(pricing.get("prompt", "0"))
            completion_price = float(pricing.get("completion", "0"))
            
            if prompt_price == 0 and completion_price == 0:
                free_models.append({
                    "id": model.get("id"),
                    "name": model.get("name", model.get("id")),
                    "contextWindow": model.get("context_length", 128000)
                })
        
        print(f"[cache] å–å¾— {len(free_models)} å€‹å…è²»æ¨¡å‹ï¼Œå¯«å…¥å¿«å–")
        # ä¿å­˜å¿«å–
        try:
            cache_data = {
                "last_check": datetime.now().isoformat(),
                "models": free_models
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"[cache] å¯«å…¥å¿«å–å¤±æ•—ï¼š{e}")
        
        return free_models
    
    except requests.RequestException as e:
        print(f"API è«‹æ±‚å¤±æ•—ï¼š{e}")
        return None


def get_current_model():
    """ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹"""
    config_path = Path.home() / ".openclaw" / "openclaw.json"
    
    try:
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            current = config.get("agents", {}).get("defaults", {}).get("model", {}).get("primary", "æœªè¨­ç½®")
            return current
    except Exception as e:
        return f"è®€å–å¤±æ•—: {e}"
    
    return "æœªæ‰¾åˆ°é…ç½®"


def analyze_models(free_models):
    """åˆ†æå…è²»æ¨¡å‹ä¸¦ç”Ÿæˆå»ºè­°"""
    if not free_models:
        return "æœªæ‰¾åˆ°å…è²»æ¨¡å‹"
    
    # æŒ‰ä¸Šä¸‹æ–‡é•·åº¦æ’åº
    sorted_by_context = sorted(free_models, key=lambda x: x['contextWindow'], reverse=True)
    
    # åˆ†é¡æ¨¡å‹
    large_context = [m for m in free_models if m['contextWindow'] >= 128000]
    vision_models = [m for m in free_models if 'vl' in m['id'].lower() or 'vision' in m['id'].lower()]
    thinking_models = [m for m in free_models if 'thinking' in m['id'].lower() or 'r1' in m['id'].lower()]
    coder_models = [m for m in free_models if 'coder' in m['id'].lower() or 'code' in m['name'].lower()]
    
    current_model = get_current_model()
    
    # ç”Ÿæˆå ±å‘Š
    report = f"""ğŸ“Š OpenRouter å…è²»æ¨¡å‹åˆ†æå ±å‘Š
ğŸ• æª¢æŸ¥æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Œ ç•¶å‰ä½¿ç”¨æ¨¡å‹:
{current_model}

ğŸ“ˆ çµ±è¨ˆè³‡è¨Š:
â€¢ ç¸½å…±æ‰¾åˆ° {len(free_models)} å€‹å…è²»æ¨¡å‹
â€¢ å¤§ä¸Šä¸‹æ–‡æ¨¡å‹ (â‰¥128K): {len(large_context)} å€‹
â€¢ è¦–è¦ºæ¨¡å‹: {len(vision_models)} å€‹
â€¢ æ€è€ƒæ¨¡å‹: {len(thinking_models)} å€‹
â€¢ ç·¨ç¨‹æ¨¡å‹: {len(coder_models)} å€‹

ğŸ† æ¨è–¦æ¨¡å‹:

1ï¸âƒ£ æœ€å¤§ä¸Šä¸‹æ–‡ (é©åˆé•·æ–‡æª”):
   {sorted_by_context[0]['name']}
   ID: {sorted_by_context[0]['id']}
   ä¸Šä¸‹æ–‡: {sorted_by_context[0]['contextWindow']:,} tokens

2ï¸âƒ£ ç·¨ç¨‹ä»»å‹™æ¨è–¦:
"""
    
    if coder_models:
        best_coder = max(coder_models, key=lambda x: x['contextWindow'])
        report += f"""   {best_coder['name']}
   ID: {best_coder['id']}
   ä¸Šä¸‹æ–‡: {best_coder['contextWindow']:,} tokens
"""
    else:
        report += "   ç„¡å°ˆé–€çš„ç·¨ç¨‹æ¨¡å‹\n"
    
    report += "\n3ï¸âƒ£ æ€è€ƒæ¨ç†ä»»å‹™:\n"
    if thinking_models:
        best_thinking = max(thinking_models, key=lambda x: x['contextWindow'])
        report += f"""   {best_thinking['name']}
   ID: {best_thinking['id']}
   ä¸Šä¸‹æ–‡: {best_thinking['contextWindow']:,} tokens
"""
    else:
        report += "   ç„¡æ€è€ƒæ¨¡å‹\n"
    
    report += "\n4ï¸âƒ£ è¦–è¦ºä»»å‹™:\n"
    if vision_models:
        best_vision = max(vision_models, key=lambda x: x['contextWindow'])
        report += f"""   {best_vision['name']}
   ID: {best_vision['id']}
   ä¸Šä¸‹æ–‡: {best_vision['contextWindow']:,} tokens
"""
    else:
        report += "   ç„¡è¦–è¦ºæ¨¡å‹\n"
    
    report += f"""
ğŸ“‹ å®Œæ•´æ¨¡å‹åˆ—è¡¨ (å‰ 10 å):
"""
    
    for i, model in enumerate(sorted_by_context[:10], 1):
        report += f"{i}. {model['name']}\n"
        report += f"   ID: {model['id']}\n"
        report += f"   ä¸Šä¸‹æ–‡: {model['contextWindow']:,}\n\n"
    
    report += f"""
ğŸ’¡ å»ºè­°:
â€¢ å¦‚æœéœ€è¦è™•ç†é•·æ–‡æª”ï¼Œå»ºè­°ä½¿ç”¨ {sorted_by_context[0]['name']}
â€¢ å¦‚æœä¸»è¦åšç·¨ç¨‹ä»»å‹™ï¼Œå»ºè­°ä½¿ç”¨ {coder_models[0]['name'] if coder_models else 'é€šç”¨æ¨¡å‹'}
â€¢ æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯å…è²»çš„ï¼Œå¯ä»¥éš¨æ™‚åˆ‡æ›æ¸¬è©¦

ğŸ”„ å¦‚éœ€æ›´æ›æ¨¡å‹ï¼Œè«‹å›è¦†æ¨¡å‹ ID
"""
    
    return report


def send_telegram_message(message):
    """ç™¼é€ Telegram è¨Šæ¯"""
    config_path = Path.home() / ".openclaw" / "openclaw.json"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        bot_token = config.get("channels", {}).get("telegram", {}).get("botToken")
        allow_from = config.get("channels", {}).get("telegram", {}).get("allowFrom", [])
        
        if not bot_token:
            print("éŒ¯èª¤ï¼šæœªæ‰¾åˆ° Telegram bot token")
            return False
        
        if not allow_from or allow_from == ["*"]:
            print("éŒ¯èª¤ï¼šæœªè¨­ç½®å…è¨±çš„ç”¨æˆ¶ ID")
            return False
        
        # ç™¼é€çµ¦æ‰€æœ‰å…è¨±çš„ç”¨æˆ¶
        success_count = 0
        for chat_id in allow_from:
            if chat_id == "*":
                continue
            
            url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
            data = {
                "chat_id": chat_id,
                "text": message,
                "parse_mode": "HTML"
            }
            
            response = requests.post(url, json=data, timeout=10)
            if response.status_code == 200:
                success_count += 1
                print(f"âœ“ å·²ç™¼é€è¨Šæ¯åˆ° {chat_id}")
            else:
                print(f"âœ— ç™¼é€åˆ° {chat_id} å¤±æ•—: {response.text}")
        
        return success_count > 0
    
    except Exception as e:
        print(f"ç™¼é€ Telegram è¨Šæ¯å¤±æ•—ï¼š{e}")
        return False


def save_models_cache(free_models):
    """ä¿å­˜æ¨¡å‹åˆ—è¡¨åˆ°ç·©å­˜"""
    cache_path = Path.home() / ".openclaw" / "free-models-cache.json"
    
    try:
        cache_data = {
            "last_check": datetime.now().isoformat(),
            "models": free_models
        }
        
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ å·²ä¿å­˜æ¨¡å‹ç·©å­˜åˆ°ï¼š{cache_path}")
        return True
    except Exception as e:
        print(f"ä¿å­˜ç·©å­˜å¤±æ•—ï¼š{e}")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    # æª¢æŸ¥æ˜¯å¦ç‚ºéœé»˜æ¨¡å¼ï¼ˆä¸ç™¼é€ Telegramï¼‰
    silent_mode = len(sys.argv) > 1 and sys.argv[1] == '--silent'
    
    print("=" * 60)
    print("OpenRouter å…è²»æ¨¡å‹åˆ†æå™¨")
    print("=" * 60)
    
    # ç²å–å…è²»æ¨¡å‹
    free_models = fetch_free_models()
    
    if free_models is None:
        error_msg = "âŒ ç„¡æ³•ç²å– OpenRouter æ¨¡å‹åˆ—è¡¨\nè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š"
        print(f"\n{error_msg}")
        if not silent_mode:
            send_telegram_message(error_msg)
        sys.exit(1)
    
    if not free_models:
        error_msg = "âŒ æœªæ‰¾åˆ°å…è²»æ¨¡å‹"
        print(f"\n{error_msg}")
        if not silent_mode:
            send_telegram_message(error_msg)
        sys.exit(1)
    
    # ä¿å­˜ç·©å­˜
    save_models_cache(free_models)
    
    # ç”Ÿæˆåˆ†æå ±å‘Š
    report = analyze_models(free_models)
    print("\n" + report)
    
    # ç™¼é€åˆ° Telegram
    if not silent_mode:
        print("\n" + "=" * 60)
        print("æ­£åœ¨ç™¼é€å ±å‘Šåˆ° Telegram...")
        if send_telegram_message(report):
            print("âœ“ å ±å‘Šå·²æˆåŠŸç™¼é€åˆ° Telegram")
        else:
            print("âœ— ç™¼é€ Telegram è¨Šæ¯å¤±æ•—")
            print("è«‹æª¢æŸ¥ ~/.openclaw/openclaw.json ä¸­çš„ Telegram é…ç½®")
    else:
        print("\n" + "=" * 60)
        print("éœé»˜æ¨¡å¼ï¼šæœªç™¼é€ Telegram è¨Šæ¯")
    
    print("=" * 60)


if __name__ == "__main__":
    main()
